<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en" lang="en">
<!-- Created by GNU Texinfo 5.1, http://www.gnu.org/software/texinfo/ -->
<head>
<title>Cau truc va Y nghia cua Chuong Trinh May tinh, 2e: 2.3.4</title>

<meta name="description" content="Cau truc va Y nghia cua Chuong Trinh May tinh, 2e: 2.3.4"/>
<meta name="keywords" content="Cau truc va Y nghia cua Chuong Trinh May tinh, 2e: 2.3.4"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<meta name="Generator" content="texi2any"/>
<meta charset="utf-8"/>
<link href="index.xhtml#Top" rel="start" title="Top"/>
<link href="Term-Index.xhtml#Term-Index" rel="index" title="Term Index"/>
<link href="index.xhtml#SEC_Contents" rel="contents" title="Table of Contents"/>
<link href="2_002e3.xhtml#g_t2_002e3" rel="prev" title="2.3"/>
<link href="2_002e4.xhtml#g_t2_002e4" rel="next" title="2.4"/>
<link href="2_002e3_002e3.xhtml#g_t2_002e3_002e3" rel="prev" title="2.3.3"/>

<link href="css/style.css" rel="stylesheet" type="text/css" />
<link href="css/prettify.css" rel="stylesheet" type="text/css" />

<script class="prettifier" src="js/highlight/prettify.js" type="text/javascript"></script>
<script class="prettifier" src="js/highlight/lang-lisp.js" type="text/javascript"></script>
</head>

<body>
<section><a id="g_t2_002e3_002e4"></a>
<nav class="header">
<p>
Next: <a href="2_002e4.xhtml#g_t2_002e4" accesskey="n" rel="next">2.4</a>, Previous: <a href="2_002e3_002e3.xhtml#g_t2_002e3_002e3" accesskey="p" rel="prev">2.3.3</a>, Up: <a href="2_002e3.xhtml#g_t2_002e3" accesskey="u" rel="prev">2.3</a> &#160; [<a href="index.xhtml#SEC_Contents" title="Table of contents" accesskey="c" rel="contents">Contents</a>][<a href="Term-Index.xhtml#Term-Index" title="Index" accesskey="i" rel="index">Index</a>]</p>
</nav>
<hr/>

<a id="Example_003a-Huffman-Encoding-Trees"></a>
<h4 class="subsection"><span class="secnum">2.3.4</span><span class="sectitle">Example: Huffman Encoding Trees</span></h4>

<p>This section provides practice in the use of list structure and data
abstraction to manipulate sets and trees.  The application is to methods for
representing data as sequences of ones and zeros (bits).  For example, the
ASCII standard code used to represent text in computers encodes each character
as a sequence of seven bits.  Using seven bits allows us to distinguish \( 2^7 \),
or 128, possible different characters.  In general, if we want to distinguish
\( n \) different symbols, we will need to use \( {\log_2n} \) bits per
symbol.  If all our messages are made up of the eight symbols A, B, C, D, E, F,
G, and H, we can choose a code with three bits per character, for example
</p>
<div class="example">
<pre class="example">A 000  C 010  E 100  G 110
B 001  D 011  F 101  H 111
</pre></div>

<p>With this code, the message
</p>
<div class="example">
<pre class="example">BACADAEAFABBAAAGAH
</pre></div>

<p>is encoded as the string of 54 bits
</p>
<div class="example">
<pre class="example">001000010000011000100000101
000001001000000000110000111
</pre></div>

<p>Codes such as ASCII and the A-through-H code above are known as
<a id="index-fixed_002dlength"></a>
<em>fixed-length</em> codes, because they represent each symbol in the message
with the same number of bits.  It is sometimes advantageous to use
<a id="index-variable_002dlength"></a>
<em>variable-length</em> codes, in which different symbols may be represented
by different numbers of bits.  For example, Morse code does not use the same
number of dots and dashes for each letter of the alphabet.  In particular, E,
the most frequent letter, is represented by a single dot.  In general, if our
messages are such that some symbols appear very frequently and some very
rarely, we can encode data more efficiently (i.e., using fewer bits per
message) if we assign shorter codes to the frequent symbols.  Consider the
following alternative code for the letters A through H:
</p>
<div class="example">
<pre class="example">A 0    C 1010  E 1100  G 1110
B 100  D 1011  F 1101  H 1111
</pre></div>

<p>With this code, the same message as above is encoded as the string
</p>
<div class="example">
<pre class="example">100010100101101100011
010100100000111001111
</pre></div>

<p>This string contains 42 bits, so it saves more than 20% in space in comparison
with the fixed-length code shown above.
</p>
<p>One of the difficulties of using a variable-length code is knowing when you
have reached the end of a symbol in reading a sequence of zeros and ones.
Morse code solves this problem by using a special <a id="index-separator-code"></a>
<em>separator code</em> (in
this case, a pause) after the sequence of dots and dashes for each letter.
Another solution is to design the code in such a way that no complete code for
any symbol is the beginning (or <a id="index-prefix"></a>
<em>prefix</em>) of the code for another
symbol.  Such a code is called a <a id="index-prefix-code"></a>
<em>prefix code</em>.  In the example above,
A is encoded by 0 and B is encoded by 100, so no other symbol can have a code
that begins with 0 or with 100.
</p>
<p>In general, we can attain significant savings if we use variable-length prefix
codes that take advantage of the relative frequencies of the symbols in the
messages to be encoded.  One particular scheme for doing this is called the
Huffman encoding method, after its discoverer, David Huffman.  A Huffman code
can be represented as a binary tree whose leaves are the symbols that are
encoded.  At each non-leaf node of the tree there is a set containing all the
symbols in the leaves that lie below the node.  In addition, each symbol at a
leaf is assigned a weight (which is its relative frequency), and each non-leaf
node contains a weight that is the sum of all the weights of the leaves lying
below it.  The weights are not used in the encoding or the decoding process.
We will see below how they are used to help construct the tree.
</p>
<p><a href="#Figure-2_002e18">Figure 2.18</a> shows the Huffman tree for the A-through-H code given above.
The weights at the leaves indicate that the tree was designed for messages in
which A appears with relative frequency 8, B with relative frequency 3, and the
other letters each with relative frequency 1.
</p>
<figure class="float">
<a id="Figure-2_002e18"></a>
<object style="width: 53.01ex; height: 35.23ex;" data="fig/chap2/Fig2.18a.std.svg" type="image/svg+xml">SVG</object>

<figcaption class="float-caption">
<p><strong>Figure 2.18:</strong> A Huffman encoding tree.</p>
</figcaption>
</figure>

<p>Given a Huffman tree, we can find the encoding of any symbol by starting at the
root and moving down until we reach the leaf that holds the symbol.  Each time
we move down a left branch we add a 0 to the code, and each time we move down a
right branch we add a 1.  (We decide which branch to follow by testing to see
which branch either is the leaf node for the symbol or contains the symbol in
its set.)  For example, starting from the root of the tree in <a href="#Figure-2_002e18">Figure 2.18</a>, 
we arrive at the leaf for D by following a right branch, then a left
branch, then a right branch, then a right branch; hence, the code for D is
1011.
</p>
<p>To decode a bit sequence using a Huffman tree, we begin at the root and use the
successive zeros and ones of the bit sequence to determine whether to move down
the left or the right branch.  Each time we come to a leaf, we have generated a
new symbol in the message, at which point we start over from the root of the
tree to find the next symbol.  For example, suppose we are given the tree above
and the sequence 10001010.  Starting at the root, we move down the right
branch, (since the first bit of the string is 1), then down the left branch
(since the second bit is 0), then down the left branch (since the third bit is
also 0).  This brings us to the leaf for B, so the first symbol of the decoded
message is B.  Now we start again at the root, and we make a left move because
the next bit in the string is 0.  This brings us to the leaf for A.  Then we
start again at the root with the rest of the string 1010, so we move right,
left, right, left and reach C.  Thus, the entire message is BAC.
</p>
<a id="Generating-Huffman-trees"></a>
<h5 class="subsubheading">Generating Huffman trees</h5>

<p>Given an “alphabet” of symbols and their relative frequencies, how do we
construct the “best” code?  (In other words, which tree will encode messages
with the fewest bits?)  Huffman gave an algorithm for doing this and showed
that the resulting code is indeed the best variable-length code for messages
where the relative frequency of the symbols matches the frequencies with which
the code was constructed.  We will not prove this optimality of Huffman codes
here, but we will show how Huffman trees are constructed.<a class="footnote_link" id="DOCF108" href="#FOOT108"><sup>108</sup></a>
</p>
<p>The algorithm for generating a Huffman tree is very simple. The idea is to
arrange the tree so that the symbols with the lowest frequency appear farthest
away from the root. Begin with the set of leaf nodes, containing symbols and
their frequencies, as determined by the initial data from which the code is to
be constructed. Now find two leaves with the lowest weights and merge them to
produce a node that has these two nodes as its left and right branches. The
weight of the new node is the sum of the two weights. Remove the two leaves
from the original set and replace them by this new node. Now continue this
process. At each step, merge two nodes with the smallest weights, removing them
from the set and replacing them with a node that has these two as its left and
right branches. The process stops when there is only one node left, which is
the root of the entire tree.  Here is how the Huffman tree of <a href="#Figure-2_002e18">Figure 2.18</a>
was generated:
</p>
<div class="example">
<pre class="example">Initial {(A 8) (B 3) (C 1) (D 1) 
leaves   (E 1) (F 1) (G 1) (H 1)}

Merge   {(A 8) (B 3) ({C D} 2) 
         (E 1) (F 1) (G 1) (H 1)}

Merge   {(A 8) (B 3) ({C D} 2) 
         ({E F} 2) (G 1) (H 1)}

Merge   {(A 8) (B 3) ({C D} 2) 
         ({E F} 2) ({G H} 2)}

Merge   {(A 8) (B 3) ({C D} 2) 
         ({E F G H} 4)}

Merge   {(A 8) ({B C D} 5) 
         ({E F G H} 4)}

Merge   {(A 8) ({B C D E F G H} 9)}

Final   {({A B C D E F G H} 17)}
merge    
</pre></div>

<p>The algorithm does not always specify a unique tree, because there may not be
unique smallest-weight nodes at each step.  Also, the choice of the order in
which the two nodes are merged (i.e., which will be the right branch and which
will be the left branch) is arbitrary.
</p>
<a id="Representing-Huffman-trees"></a>
<h5 class="subsubheading">Representing Huffman trees</h5>

<p>In the exercises below we will work with a system that uses Huffman trees to
encode and decode messages and generates Huffman trees according to the
algorithm outlined above.  We will begin by discussing how trees are
represented.
</p>
<p>Leaves of the tree are represented by a list consisting of the symbol
<code>leaf</code>, the symbol at the leaf, and the weight:
</p>
<div class="lisp">
<pre class="lisp">(define (make-leaf symbol weight)
  (list 'leaf symbol weight))
(define (leaf? object)
  (eq? (car object) 'leaf))
(define (symbol-leaf x) (cadr x))
(define (weight-leaf x) (caddr x))
</pre></div>

<p>A general tree will be a list of a left branch, a right branch, a set of
symbols, and a weight.  The set of symbols will be simply a list of the
symbols, rather than some more sophisticated set representation.  When we make
a tree by merging two nodes, we obtain the weight of the tree as the sum of the
weights of the nodes, and the set of symbols as the union of the sets of
symbols for the nodes.  Since our symbol sets are represented as lists, we can
form the union by using the <code>append</code> procedure we defined in 
<a href="2_002e2_002e1.xhtml#g_t2_002e2_002e1">2.2.1</a>:
</p>
<div class="lisp">
<pre class="lisp">(define (make-code-tree left right)
  (list left
        right
        (append (symbols left) 
                (symbols right))
        (+ (weight left) (weight right))))
</pre></div>

<p>If we make a tree in this way, we have the following selectors:
</p>
<div class="lisp">
<pre class="lisp">(define (left-branch tree) (car tree))
(define (right-branch tree) (cadr tree))

(define (symbols tree)
  (if (leaf? tree)
      (list (symbol-leaf tree))
      (caddr tree)))

(define (weight tree)
  (if (leaf? tree)
      (weight-leaf tree)
      (cadddr tree)))
</pre></div>

<p>The procedures <code>symbols</code> and <code>weight</code> must do something slightly
different depending on whether they are called with a leaf or a general tree.
These are simple examples of <a id="index-generic-procedures"></a>
<em>generic procedures</em> (procedures that can
handle more than one kind of data), which we will have much more to say about
in <a href="2_002e4.xhtml#g_t2_002e4">2.4</a> and <a href="2_002e5.xhtml#g_t2_002e5">2.5</a>.
</p>
<a id="The-decoding-procedure"></a>
<h5 class="subsubheading">The decoding procedure</h5>

<p>The following procedure implements the decoding algorithm.  It takes as
arguments a list of zeros and ones, together with a Huffman tree.
</p>
<div class="lisp">
<pre class="lisp">(define (decode bits tree)
  (define (decode-1 bits current-branch)
    (if (null? bits)
        '()
        (let ((next-branch
               (choose-branch 
                (car bits) 
                current-branch)))
          (if (leaf? next-branch)
              (cons 
               (symbol-leaf next-branch)
               (decode-1 (cdr bits) tree))
              (decode-1 (cdr bits) 
                        next-branch)))))
  (decode-1 bits tree))

(define (choose-branch bit branch)
  (cond ((= bit 0) (left-branch branch))
        ((= bit 1) (right-branch branch))
        (else (error &quot;bad bit: 
               CHOOSE-BRANCH&quot; bit))))
</pre></div>

<p>The procedure <code>decode-1</code> takes two arguments: the list of remaining bits
and the current position in the tree.  It keeps moving “down” the tree,
choosing a left or a right branch according to whether the next bit in the list
is a zero or a one.  (This is done with the procedure <code>choose-branch</code>.)
When it reaches a leaf, it returns the symbol at that leaf as the next symbol
in the message by <code>cons</code>ing it onto the result of decoding the rest of the
message, starting at the root of the tree.  Note the error check in the final
clause of <code>choose-branch</code>, which complains if the procedure finds
something other than a zero or a one in the input data.
</p>
<a id="Sets-of-weighted-elements"></a>
<h5 class="subsubheading">Sets of weighted elements</h5>

<p>In our representation of trees, each non-leaf node contains a set of symbols,
which we have represented as a simple list.  However, the tree-generating
algorithm discussed above requires that we also work with sets of leaves and
trees, successively merging the two smallest items.  Since we will be required
to repeatedly find the smallest item in a set, it is convenient to use an
ordered representation for this kind of set.
</p>
<p>We will represent a set of leaves and trees as a list of elements, arranged in
increasing order of weight.  The following <code>adjoin-set</code> procedure for
constructing sets is similar to the one described in <a href="2_002e3_002e3.xhtml#Exercise-2_002e61">Exercise 2.61</a>;
however, items are compared by their weights, and the element being added to
the set is never already in it.
</p>
<div class="lisp">
<pre class="lisp">(define (adjoin-set x set)
  (cond ((null? set) (list x))
        ((&lt; (weight x) (weight (car set))) 
         (cons x set))
        (else 
         (cons (car set)
               (adjoin-set x (cdr set))))))
</pre></div>

<p>The following procedure takes a list of symbol-frequency pairs such as
<code>((A 4) (B 2) (C 1) (D 1))</code> and constructs an initial ordered set of
leaves, ready to be merged according to the Huffman algorithm:
</p>
<div class="lisp">
<pre class="lisp">(define (make-leaf-set pairs)
  (if (null? pairs)
      '()
      (let ((pair (car pairs)))
        (adjoin-set 
         (make-leaf (car pair)    <span class="roman">; symbol</span>
                    (cadr pair))  <span class="roman">; frequency</span>
         (make-leaf-set (cdr pairs))))))
</pre></div>

<blockquote>
<p><strong><a id="Exercise-2_002e67"></a>Exercise 2.67:</strong> Define an encoding tree and a
sample message:
</p>
<div class="lisp">
<pre class="lisp">(define sample-tree
  (make-code-tree 
   (make-leaf 'A 4)
   (make-code-tree
    (make-leaf 'B 2)
    (make-code-tree 
     (make-leaf 'D 1)
     (make-leaf 'C 1)))))

(define sample-message 
  '(0 1 1 0 0 1 0 1 0 1 1 1 0))
</pre></div>

<p>Use the <code>decode</code> procedure to decode the message, and give the result.
</p></blockquote>

<blockquote>
<p><strong><a id="Exercise-2_002e68"></a>Exercise 2.68:</strong> The <code>encode</code> procedure takes
as arguments a message and a tree and produces the list of bits that gives the
encoded message.
</p>
<div class="lisp">
<pre class="lisp">(define (encode message tree)
  (if (null? message)
      '()
      (append 
       (encode-symbol (car message) 
                      tree)
       (encode (cdr message) tree))))
</pre></div>

<p><code>Encode-symbol</code> is a procedure, which you must write, that returns the
list of bits that encodes a given symbol according to a given tree.  You should
design <code>encode-symbol</code> so that it signals an error if the symbol is not in
the tree at all.  Test your procedure by encoding the result you obtained in
<a href="#Exercise-2_002e67">Exercise 2.67</a> with the sample tree and seeing whether it is the same as
the original sample message.
</p></blockquote>

<blockquote>
<p><strong><a id="Exercise-2_002e69"></a>Exercise 2.69:</strong> The following procedure takes as
its argument a list of symbol-frequency pairs (where no symbol appears in more
than one pair) and generates a Huffman encoding tree according to the Huffman
algorithm.
</p>
<div class="lisp">
<pre class="lisp">(define (generate-huffman-tree pairs)
  (successive-merge 
   (make-leaf-set pairs)))
</pre></div>

<p><code>Make-leaf-set</code> is the procedure given above that transforms the list of
pairs into an ordered set of leaves.  <code>Successive-merge</code> is the procedure
you must write, using <code>make-code-tree</code> to successively merge the
smallest-weight elements of the set until there is only one element left, which
is the desired Huffman tree.  (This procedure is slightly tricky, but not
really complicated.  If you find yourself designing a complex procedure, then
you are almost certainly doing something wrong.  You can take significant
advantage of the fact that we are using an ordered set representation.)
</p></blockquote>

<blockquote>
<p><strong><a id="Exercise-2_002e70"></a>Exercise 2.70:</strong> The following eight-symbol
alphabet with associated relative frequencies was designed to efficiently
encode the lyrics of 1950s rock songs.  (Note that the “symbols” of an
“alphabet” need not be individual letters.)
</p>
<div class="example">
<pre class="example">
A    2    NA  16
BOOM 1    SHA  3
GET  2    YIP  9
JOB  2    WAH  1
</pre></div>

<p>Use <code>generate-huffman-tree</code> (<a href="#Exercise-2_002e69">Exercise 2.69</a>) to generate a
corresponding Huffman tree, and use <code>encode</code> (<a href="#Exercise-2_002e68">Exercise 2.68</a>) to
encode the following message:
</p>
<div class="example">
<pre class="example">Get a job
Sha na na na na na na na na

Get a job
Sha na na na na na na na na

Wah yip yip yip yip 
yip yip yip yip yip
Sha boom
</pre></div>

<p>How many bits are required for the encoding?  What is the smallest number of
bits that would be needed to encode this song if we used a fixed-length code
for the eight-symbol alphabet?
</p></blockquote>

<blockquote>
<p><strong><a id="Exercise-2_002e71"></a>Exercise 2.71:</strong> Suppose we have a Huffman tree
for an alphabet of \( n \) symbols, and that the relative frequencies of the
symbols are \( {1, 2, 4, \dots, 2^{n-1}} \).  Sketch the tree for \( {n=5} \); for
\( {n=10} \).  In such a tree (for general \( n \)) how many bits are required to
encode the most frequent symbol?  The least frequent symbol?
</p></blockquote>

<blockquote>
<p><strong><a id="Exercise-2_002e72"></a>Exercise 2.72:</strong> Consider the encoding procedure
that you designed in <a href="#Exercise-2_002e68">Exercise 2.68</a>.  What is the order of growth in the
number of steps needed to encode a symbol?  Be sure to include the number of
steps needed to search the symbol list at each node encountered.  To answer
this question in general is difficult.  Consider the special case where the
relative frequencies of the \( n \) symbols are as described in <a href="#Exercise-2_002e71">Exercise 2.71</a>, 
and give the order of growth (as a function of \( n \)) of the number of
steps needed to encode the most frequent and least frequent symbols in the
alphabet.
</p></blockquote>

<div class="footnote">
<hr/>

<h4 class="footnotes-heading">Footnotes</h4>

<div id="FOOT108"><p><a class="footnote_backlink" href="#DOCF108"><sup>108</sup></a>
See <a href="References.xhtml#Hamming-1980">Hamming 1980</a> 
for a discussion of the mathematical properties of Huffman codes.</p>
</div>
</div>
<hr/>
<nav class="header">
<p>
Next: <a href="2_002e4.xhtml#g_t2_002e4" accesskey="n" rel="next">2.4</a>, Previous: <a href="2_002e3_002e3.xhtml#g_t2_002e3_002e3" accesskey="p" rel="prev">2.3.3</a>, Up: <a href="2_002e3.xhtml#g_t2_002e3" accesskey="u" rel="prev">2.3</a> &#160; [<a href="index.xhtml#SEC_Contents" title="Table of contents" accesskey="c" rel="contents">Contents</a>][<a href="Term-Index.xhtml#Term-Index" title="Index" accesskey="i" rel="index">Index</a>]</p>
</nav>


</section>
</body>
</html>
